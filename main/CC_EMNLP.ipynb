{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5426212241734430929ff8b86a36c8be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd306595c8c84c25984c8e8e736daed0",
              "IPY_MODEL_ebf941b0e7094b86af5b885c84d7d74c",
              "IPY_MODEL_1ac2a299ae13419cb3347a991fead4e8"
            ],
            "layout": "IPY_MODEL_4b03771dd9c84a6a9b7b36d402a456c4"
          }
        },
        "fd306595c8c84c25984c8e8e736daed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b36039246e348df93311d003e36e19e",
            "placeholder": "​",
            "style": "IPY_MODEL_930e06542bd2427795beaf7f278555ed",
            "value": "100%"
          }
        },
        "ebf941b0e7094b86af5b885c84d7d74c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0a7f2e3efdb4e17bd47b5c720bd78cb",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c04c8399799a4525b7b37b5cb1f5f284",
            "value": 10
          }
        },
        "1ac2a299ae13419cb3347a991fead4e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_044e015bcbe14ba69f2ee74cc8f7a61a",
            "placeholder": "​",
            "style": "IPY_MODEL_ce5c135443fd43a9b97247571ddeef82",
            "value": " 10/10 [00:22&lt;00:00,  2.12s/it]"
          }
        },
        "4b03771dd9c84a6a9b7b36d402a456c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b36039246e348df93311d003e36e19e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "930e06542bd2427795beaf7f278555ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0a7f2e3efdb4e17bd47b5c720bd78cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c04c8399799a4525b7b37b5cb1f5f284": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "044e015bcbe14ba69f2ee74cc8f7a61a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce5c135443fd43a9b97247571ddeef82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# MACROS\n",
        "MIN_DEGREE = 0\n",
        "ITERATIONS = 1\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "import networkx as nx\n",
        "from statistics import mean, median\n",
        "import inspect\n",
        "import queue as Q\n",
        "from time import process_time\n",
        "from typing import List, Tuple, Set\n",
        "from random import shuffle, random, choices, randrange\n",
        "import time\n",
        "from math import ceil\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "class Graph(nx.Graph):\n",
        "    def is_multilabel_graph(self):\n",
        "        return False\n",
        "    \n",
        "    def without_secondary_edges(self):\n",
        "        edited_graph = Graph()\n",
        "        edited_graph.add_nodes_from(self.nodes())\n",
        "        primary_colors = dict([(i, primary_color(self, i)) for i in self.nodes()])\n",
        "        for u, v in self.edges():\n",
        "            if primary_colors[u] == primary_colors[v] and primary_colors[u] in self.colors_of(u, v):\n",
        "                edited_graph.add_edge(u, v, color=primary_colors[u])\n",
        "        return edited_graph\n",
        "        \n",
        "    def primary_edge_graph(self):\n",
        "        if not hasattr(self, 'primary_graph'):\n",
        "            self.primary_graph = self.without_secondary_edges()\n",
        "        return self.primary_graph\n",
        "\n",
        "    def colors_of(self, a: int, b: int) -> List[int]:\n",
        "        return [self.color_of(a, b)]\n",
        "    \n",
        "    def color_of(self, a: int, b: int) -> int:\n",
        "        return self.edges[(a,b)]['color']\n",
        "\n",
        "    def colors(self) -> Set[int]:\n",
        "        if not hasattr(self, 'color_set'):\n",
        "            colors = set()\n",
        "            for edge in self.edges:\n",
        "                colors.add(self.edges[edge]['color'])\n",
        "            self.color_set = colors\n",
        "        return self.color_set\n",
        "\n",
        "    def node_pairs(self) -> List[Tuple[int, int]]:\n",
        "        return [(a, b) for a in self.nodes for b in self.nodes if a < b]\n",
        "    \n",
        "    def is_valid_clustering(self, clustering: List[Tuple[List[int], int]]) -> bool:\n",
        "        cluster_nodes = set()\n",
        "        for cluster, col in clustering:\n",
        "            if not isinstance(col, int):\n",
        "                print(f\"Cluster col {col} is not an integer.\")\n",
        "                print_cluster_head(cluster, 20)\n",
        "                return False\n",
        "            for node in cluster:\n",
        "                if node in cluster_nodes:\n",
        "                    print(f\"{node} is already clustered in some other cluster.\")\n",
        "                    print_cluster_head(cluster, 20)\n",
        "                    return False\n",
        "                cluster_nodes.add(node)\n",
        "        if cluster_nodes.symmetric_difference(set(self.nodes())):\n",
        "            print(f\"Set of clustered nodes is not equal to set of nodes.\")\n",
        "            print(f\"{len(cluster_nodes)} were clustered, {len(self.nodes())} nodes in this graph.\")\n",
        "            print(f\"{len(cluster_nodes.symmetric_difference(set(self.nodes())))} nodes in difference.\")\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def error_of(self, clustering: List[Tuple[List[int], int]]) -> int:\n",
        "        if not self.is_valid_clustering(clustering):\n",
        "            raise Exception('Clustering is not valid')\n",
        "        remaining_edges = self.number_of_edges()\n",
        "        color_errors = 0\n",
        "        non_edge_errors = 0\n",
        "        for cluster, color in clustering:\n",
        "            # print(\"color: \",color)\n",
        "            for v in cluster:\n",
        "                # print(\"---v: \",v)\n",
        "                for u in cluster:\n",
        "                    # print(\"u: \",u)\n",
        "                    if u <= v: continue\n",
        "                    if not self.has_edge(u, v):\n",
        "                        non_edge_errors += 1\n",
        "                        # print(\"inside violation on \",u,v)\n",
        "                    else:\n",
        "                        remaining_edges -= 1\n",
        "                        if self.color_of(u,v) != color:\n",
        "                            color_errors += 1\n",
        "                            # print(\"color violation on \",u,v)\n",
        "        return remaining_edges + non_edge_errors + color_errors\n",
        "\n",
        "def most_frequent_color(graph: Graph, vertices: List[int]):\n",
        "    count = {0: 0}\n",
        "    for a in vertices:\n",
        "        for b in vertices:\n",
        "            if not a < b or not graph.has_edge(a, b): continue\n",
        "            for color in graph.colors_of(a,b):\n",
        "                count[color] = count.get(color, 0) + 1\n",
        "    return max(count, key=count.get)\n",
        "\n",
        "def cluster_ids(clustering: List[Tuple[List[int], int]]):\n",
        "    cluster_id = {}\n",
        "    for i, cluster in enumerate(clustering):\n",
        "        for node in cluster[0]:\n",
        "            cluster_id[node] = i\n",
        "    return cluster_id\n",
        "\n",
        "def intersect_clusterings(clustering_1: List[Tuple[List[int], int]], clustering_2: List[Tuple[List[int], int]]):\n",
        "    cluster_ids_1 = cluster_ids(clustering_1)\n",
        "    cluster_ids_2 = cluster_ids(clustering_2)\n",
        "    clustering = {}\n",
        "    for node, id1 in cluster_ids_1.items():\n",
        "        id2 = cluster_ids_2[node]\n",
        "        if not (id1, id2) in clustering:\n",
        "            clustering[(id1, id2)] = ([], clustering_1[id1][1])\n",
        "        clustering[(id1, id2)][0].append(node)\n",
        "    return list(clustering.values())\n",
        "\n",
        "def shuffled(collection):\n",
        "    as_list = [i for i in collection]\n",
        "    shuffle(as_list)\n",
        "    return as_list\n",
        "\n",
        "def vote(graph: Graph):\n",
        "    graph = graph.primary_edge_graph()\n",
        "    cluster_of = {}\n",
        "    clustering = []\n",
        "    cluster_colors = []\n",
        "    for node in shuffled(graph.nodes()):\n",
        "    # for node in graph.nodes():\n",
        "        connectivity = {}\n",
        "        # print(\"for1 node: \",node)\n",
        "        # print(\"for1 adding naghbors\")\n",
        "        for u in graph.neighbors(node):\n",
        "            # print(\"neighbors: \",u)\n",
        "            if u not in cluster_of: continue\n",
        "            # print(\"node: \" + str(u) + \" not clustered\")\n",
        "            cluster = cluster_of[u]\n",
        "            connectivity[cluster] = connectivity.get(cluster, 0) + 1\n",
        "        # print(\"end of adding naghbors\")\n",
        "        # print(\" \".join(str(connectivity)))\n",
        "        best_cluster = max(connectivity, key = lambda id: 2*connectivity[id] - len(clustering[id]) + random()*0.42) if connectivity else None\n",
        "        # print(\"best_cluster for\" + str(u) + \" is \" + str(best_cluster))\n",
        "        if best_cluster is not None and 2*connectivity[best_cluster] - len(clustering[best_cluster]) > 0:\n",
        "            clustering[best_cluster].append(node)\n",
        "            cluster_of[node] = best_cluster\n",
        "        else:\n",
        "            clustering.append([node])\n",
        "            cluster_colors.append(primary_color(graph, node))\n",
        "            cluster_of[node] = len(clustering) - 1\n",
        "    return list(zip(clustering, cluster_colors))\n",
        "\n",
        "def vote_no_rand(graph: Graph):\n",
        "    graph = graph.primary_edge_graph()\n",
        "    cluster_of = {}\n",
        "    clustering = []\n",
        "    cluster_colors = []\n",
        "    for node in shuffled(graph.nodes()):\n",
        "    # for node in graph.nodes():\n",
        "        connectivity = {}\n",
        "        for u in graph.neighbors(node):\n",
        "            if u not in cluster_of: continue\n",
        "            cluster = cluster_of[u]\n",
        "            connectivity[cluster] = connectivity.get(cluster, 0) + 1\n",
        "        best_cluster = max(connectivity, key = lambda id: 2*connectivity[id] - len(clustering[id])) if connectivity else None\n",
        "        if best_cluster is not None and 2*connectivity[best_cluster] - len(clustering[best_cluster]) > 0:\n",
        "            clustering[best_cluster].append(node)\n",
        "            cluster_of[node] = best_cluster\n",
        "        else:\n",
        "            clustering.append([node])\n",
        "            cluster_colors.append(primary_color(graph, node))\n",
        "            cluster_of[node] = len(clustering) - 1\n",
        "    return list(zip(clustering, cluster_colors))\n",
        "\n",
        "def vote_choose_best(graph: Graph):\n",
        "    graph = graph.primary_edge_graph()\n",
        "    H = Graph()\n",
        "    H.add_nodes_from(sorted(graph.nodes(data=True)))\n",
        "    H.add_edges_from(graph.edges(data=True))\n",
        "    cluster_of = {}\n",
        "    clustering = []\n",
        "    cluster_colors = []  \n",
        "    # adding node 0 to cluster 0\n",
        "    choosed_cluster = 0\n",
        "    choosed_node = 0\n",
        "    clustering.append([choosed_node])\n",
        "    cluster_of[choosed_node] = choosed_cluster\n",
        "\n",
        "    for _ in H.nodes(): \n",
        "        choosed_cluster = None\n",
        "        choosed_node_error = -1\n",
        "        for v2 in H.nodes():\n",
        "            if v2 not in cluster_of: \n",
        "                connectivity = {}\n",
        "                for nbr_v2 in H.neighbors(v2):\n",
        "                    if nbr_v2 not in cluster_of: continue\n",
        "                    cluster = cluster_of[nbr_v2]\n",
        "                    connectivity[cluster] = connectivity.get(cluster, 0) + 1\n",
        "                best_cluster = max(connectivity, key = lambda id: 2*connectivity[id] - len(clustering[id])) if connectivity else None\n",
        "                \n",
        "                if best_cluster is not None and 2*connectivity[best_cluster] - len(clustering[best_cluster]) > 0:\n",
        "                    temp_error = 2*connectivity[best_cluster] - len(clustering[best_cluster])\n",
        "                else:\n",
        "                    temp_error = -1     \n",
        "\n",
        "                if (temp_error > choosed_node_error):\n",
        "                    choosed_node_error = temp_error\n",
        "                    choosed_node = v2\n",
        "                    choosed_cluster = best_cluster\n",
        "        if choosed_cluster is not None:\n",
        "            clustering[choosed_cluster].append(choosed_node)\n",
        "            cluster_of[choosed_node] = choosed_cluster\n",
        "            # print(\"node \" +str(choosed_node) + \" added to cluster \" + str(choosed_cluster))\n",
        "        else:\n",
        "            for v3 in H.nodes():\n",
        "                if v3 not in cluster_of: \n",
        "                    choosed_node = v3\n",
        "                    break\n",
        "            clustering.append([choosed_node])\n",
        "            cluster_colors.append(primary_color(H, choosed_node))\n",
        "            cluster_of[choosed_node] = len(clustering) - 1\n",
        "            # print(\"node \" +str(choosed_node) + \" added to own cluster \" + str(cluster_of[choosed_node]))\n",
        "    return list(zip(clustering, cluster_colors))\n",
        "\n",
        "def pivot(graph: Graph): # 3-Approximation\n",
        "    clustering = []\n",
        "    is_clustered = dict((i, False) for i in graph.nodes())\n",
        "    for center in shuffled(graph.nodes()):\n",
        "        if is_clustered[center]: continue\n",
        "        is_clustered[center] = True\n",
        "        cluster = [center]\n",
        "        for a in graph.neighbors(center):\n",
        "            if is_clustered[a]: continue\n",
        "            is_clustered[a] = True\n",
        "            cluster.append(a)\n",
        "        cluster_color = most_frequent_color(graph, cluster)\n",
        "        clustering.append((cluster, cluster_color))\n",
        "    return clustering\n",
        "\n",
        "def reduce_and_cluster(graph:Graph): # 5-Approximation\n",
        "    graph = graph.primary_edge_graph()\n",
        "    return pivot(graph)\n",
        "\n",
        "def read_dataset(name: str):\n",
        "    if name in ['facebook', 'twitter', 'microsoft_academic','facebook_multilabel', 'twitter_multilabel', 'microsoft_academic_multilabel']:\n",
        "        return map(remove_self_loops, read_small_dataset(name))\n",
        "    data_dict = {\n",
        "        'dawn': (lambda : read_hyperedge('DAWN_majority.csv')),\n",
        "        'cooking': (lambda : read_hyperedge('Cooking_majority.csv')),\n",
        "        'legacy_dblp': (lambda : read_legacy('DBLP_ALL.csv')),\n",
        "        'legacy_string': (lambda : read_legacy('STRING_ALL.csv')),\n",
        "        '6_rows': (lambda : read_legacy('new_6_rows.csv')),\n",
        "        '7_rows': (lambda : read_legacy('new_7_rows.csv')),\n",
        "        '8_rows': (lambda : read_legacy('new_8_rows.csv')),\n",
        "        '9_rows': (lambda : read_legacy('new_9_rows.csv')),\n",
        "        '10_rows': (lambda : read_legacy('new_10_rows.csv')),\n",
        "        '100_rows': (lambda : read_legacy('new_100_rows.csv')),\n",
        "        '200_rows': (lambda : read_legacy('new_200_rows.csv')),\n",
        "        '300_rows': (lambda : read_legacy('new_300_rows.csv')),\n",
        "        '1000_rows': (lambda : read_legacy('new_1000_rows.csv')),\n",
        "    }\n",
        "    if name not in data_dict:\n",
        "        raise Exception('Unknown Dataset')\n",
        "    return map(remove_self_loops, data_dict[name]())\n",
        "\n",
        "def remove_self_loops(graph):\n",
        "    for node in graph.nodes():\n",
        "        if graph.has_edge(node, node):\n",
        "            graph.remove_edge(node, node)\n",
        "    return graph\n",
        "\n",
        "def read_social_circles(path, multilabel = False):\n",
        "    graph = None\n",
        "    if multilabel:\n",
        "        graph = nx.read_edgelist(join(path, 'combined.txt'), nodetype = int, create_using = MultiGraph, comments = '#')\n",
        "    else:\n",
        "        graph = nx.read_edgelist(join(path, 'combined.txt'), nodetype=int, create_using = Graph,comments ='#')\n",
        "    circle_path = join(path, 'circles')\n",
        "    circle_files = [f for f in os.listdir(circle_path) if isfile(join(circle_path, f))]\n",
        "    circles_of = {}\n",
        "    circle_id = 0\n",
        "    for circle_file in circle_files:\n",
        "        with open(join(circle_path, circle_file), 'r') as file:\n",
        "            ego = os.path.basename(circle_file).split('.')[0]\n",
        "            for line in file.readlines():\n",
        "                circle = [int(a) for a in line.split()[1:]]\n",
        "                circle.append(ego)\n",
        "                for node in circle:\n",
        "                    if not node in circles_of:\n",
        "                        circles_of[node] = set()\n",
        "                    circles_of[node].add(circle_id)\n",
        "                circle_id += 1\n",
        "    random_edges = 0\n",
        "    semi_random_edges = 0\n",
        "    useless_nodes = [node for node in graph.nodes() if node not in circles_of]\n",
        "    graph.remove_nodes_from(useless_nodes)\n",
        "    for a, b in graph.edges():\n",
        "        shared_circles = circles_of[a].intersection(circles_of[b])\n",
        "        edge_color = None\n",
        "        if multilabel:\n",
        "            if len(shared_circles) == 0:\n",
        "                shared_circles = [random.randrange(circle_id)]\n",
        "                random_edges += 1  \n",
        "            graph.edges[(a,b)]['colors'] = list(shared_circles)\n",
        "        else:\n",
        "            if shared_circles:\n",
        "                edge_color = random.choice(list(shared_circles))\n",
        "                if len(shared_circles) > 1: semi_random_edges += 1\n",
        "            else:\n",
        "                edge_color = random.randrange(circle_id)   \n",
        "                random_edges += 1      \n",
        "            graph.edges[(a, b)]['color'] = edge_color\n",
        "    return [graph]\n",
        "                \n",
        "def read_microsoft_academic(multilabel = False):\n",
        "    data_path = './data/microsoft_academic/'\n",
        "    labels = []\n",
        "    with open(join(data_path,'hyperedge-labels.txt')) as file:\n",
        "        for line in file.readlines():\n",
        "            labels.append(int(line)-1)\n",
        "    edge_candidates = {}\n",
        "    with open(join(data_path,'hyperedges.txt')) as file:\n",
        "        for index, line in enumerate(file.readlines()):\n",
        "            label = labels[index]\n",
        "            authors = [int(i) for i in line.split()]\n",
        "            for a in range(len(authors)):\n",
        "                for b in range(a+1, len(authors)):\n",
        "                    adrian = authors[a]\n",
        "                    bdrian = authors[b]\n",
        "                    edge = (min(adrian, bdrian), max(adrian, bdrian))\n",
        "                    if edge not in edge_candidates:\n",
        "                        edge_candidates[edge] = {}\n",
        "                    edge_candidates[edge][label] = edge_candidates[edge].get(label, 0) + 1\n",
        "    graph = MultiGraph() if multilabel else Graph()\n",
        "    for edge, potential_topics in edge_candidates.items():\n",
        "        if multilabel:\n",
        "            graph.add_edge(edge[0], edge[1], colors = list(potential_topics.keys()))\n",
        "        else:\n",
        "            most_frequent_topic = max(potential_topics, key = potential_topics.get)\n",
        "            graph.add_edge(edge[0], edge[1], color = most_frequent_topic)\n",
        "    return [graph]\n",
        "\n",
        "def spliturl(url):\n",
        "    splitted = url.split('/')\n",
        "    if len(splitted) >= 3:\n",
        "        return splitted[2]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def generate_dblp():\n",
        "    data_path = './data/dblp/'\n",
        "    publication_types = ['output_article', 'output_inproceedings']\n",
        "    for publication_type in publication_types:\n",
        "        print(publication_type)\n",
        "        headername = publication_type + '_header.csv'\n",
        "        df_header = pd.read_csv(join(data_path, headername), sep =';')\n",
        "        column_names = ([i.split(':')[0] for i in df_header.columns])\n",
        "        if 'url' not in column_names:\n",
        "            print('does not have a url: ', column_names)\n",
        "            continue\n",
        "        filename = publication_type + '.csv'\n",
        "        column_types = {name:str for name in column_names}\n",
        "        df = pd.read_csv(join(data_path, filename), sep = ';', header = None, names = column_names, dtype=column_types)\n",
        "        df = df[['author', 'url']]\n",
        "        print(df.shape)\n",
        "        df = df.dropna()\n",
        "        df['url'] = df['url'].apply(spliturl)\n",
        "        df = df.dropna()\n",
        "        print(df.shape)\n",
        "        df.to_csv(join(data_path, publication_type + '_dataset.csv'))\n",
        "        print(df.head())\n",
        "\n",
        "def read_dblp_slow(multilabel = False):\n",
        "    data_path = './data/dblp_original/'\n",
        "    publication_types = ['output_article', 'output_inproceedings']\n",
        "    authors_by_name = {}\n",
        "    journals_by_name = {}\n",
        "    author_id_counter = 0\n",
        "    journal_id_counter = 0\n",
        "    edge_candidates = {}\n",
        "    for publication_type in publication_types:\n",
        "        print(publication_type)\n",
        "        filename = join(data_path, publication_type + '_dataset.csv')\n",
        "        with open(filename, 'r', encoding='utf-8') as file: \n",
        "            line = file.readline() #header\n",
        "            line = file.readline()\n",
        "            while line:\n",
        "                authors = line.split(',')[1].split('|')\n",
        "                journal = line.split(',')[2]\n",
        "                if journal not in journals_by_name:\n",
        "                    journals_by_name[journal] = journal_id_counter\n",
        "                    journal_id_counter+= 1\n",
        "                for author in authors:\n",
        "                    if author not in authors_by_name:\n",
        "                        authors_by_name[author] = author_id_counter\n",
        "                        author_id_counter+= 1\n",
        "                journal_id = journals_by_name[journal]\n",
        "                for i in range(len(authors)):\n",
        "                    for j in range(i+1, len(authors)):\n",
        "                        idA = authors_by_name[authors[i]]\n",
        "                        idB = authors_by_name[authors[j]]\n",
        "                        if idA == idB:\n",
        "                            continue\n",
        "                        edge = (min(idA, idB), max(idA, idB))\n",
        "                        if edge not in edge_candidates:\n",
        "                            edge_candidates[edge] = {}\n",
        "                        if not journal_id in edge_candidates[edge]:\n",
        "                            edge_candidates[edge][journal_id] = 0\n",
        "                        edge_candidates[edge][journal_id] += 1\n",
        "                line = file.readline()        \n",
        "    graph = MultiGraph() if multilabel else Graph()\n",
        "    for edge, potential_topics in edge_candidates.items():\n",
        "        if multilabel:\n",
        "            graph.add_edge(edge[0], edge[1], colors = potential_topics.keys())\n",
        "        else:\n",
        "            most_frequent_topic = max(potential_topics, key = potential_topics.get)\n",
        "            graph.add_edge(edge[0], edge[1], color = most_frequent_topic)\n",
        "    return [graph]\n",
        "\n",
        "def read_dblp(multilabel = False):\n",
        "    graph = None\n",
        "    if multilabel:\n",
        "        filename = \"./data/dblp/dblp_multilabel.edgelist\"\n",
        "        graph = MultiGraph()\n",
        "        with open(filename) as file:\n",
        "            for index, line in enumerate(file.readlines()):\n",
        "                parts = line.split(' ', maxsplit = 2)\n",
        "                a = int(parts[0])\n",
        "                b = int(parts[1])\n",
        "                color_list = parts[2].split('[', maxsplit = 1)[1][:-3]\n",
        "                colors = set(int(i) for i in color_list.split(','))\n",
        "                graph.add_edge(a,b, colors = colors)\n",
        "    else:\n",
        "        graph = nx.read_edgelist(\"dblp.edgelist\", create_using = Graph,nodetype =int, data=((\"color\", int),))\n",
        "    return [graph]\n",
        "\n",
        "def read_string(multilabel = False):\n",
        "    def read_graph(path):\n",
        "        graph = Graph() if not multilabel else MultiGraph()\n",
        "        with open(path) as file:\n",
        "            line = file.readline()\n",
        "            line = file.readline()\n",
        "            while line:\n",
        "                elems = [int(x) for x in line.strip().split(',')]\n",
        "                u = elems[0]\n",
        "                v = elems[1]\n",
        "                graph.add_edge(u, v)\n",
        "                if not multilabel:\n",
        "                    graph[u][v]['color'] = elems[2]\n",
        "                else:\n",
        "                    graph[u][v]['colors'] = set(elems[2:])\n",
        "                line = file.readline()\n",
        "        return graph\n",
        "\n",
        "    directory = './data/string_protein'\n",
        "    return (read_graph(os.path.join(directory, filename)) for filename in os.listdir(directory))\n",
        "\n",
        "def read_hyperedge(filename, multilabel = False):\n",
        "    def read_graph(path):\n",
        "        graph = Graph() if not multilabel else MultiGraph()\n",
        "        with open(path) as file:\n",
        "            line = file.readline()\n",
        "            while line:\n",
        "                elems = [int(x) for x in line.strip().split(',')]\n",
        "                u = elems[0]\n",
        "                v = elems[1]\n",
        "                graph.add_edge(u, v)\n",
        "                if not multilabel:\n",
        "                    graph[u][v]['color'] = elems[2]\n",
        "                else:\n",
        "                    graph[u][v]['colors'] = set(elems[2:])\n",
        "                line = file.readline()\n",
        "        return graph\n",
        "\n",
        "    # path = Path(__file__).parent / filename\n",
        "    path = filename\n",
        "\n",
        "    return [read_graph(path)]\n",
        "\n",
        "def read_legacy(filename):\n",
        "    def read_graph(path):\n",
        "        return nx.read_edgelist(path, comments='#', delimiter=' ', create_using=Graph,nodetype =int, data=[('color', int)])\n",
        "\n",
        "    # path = Path(__file__).parent / filename\n",
        "    path = filename\n",
        "    return [read_graph(path)]\n",
        "\n",
        "def read_small_dataset(filename):\n",
        "    def read_graph(path, multilabel = False):\n",
        "        print(\"answer: \")\n",
        "        print(path)\n",
        "        if multilabel:\n",
        "            graph = MultiGraph()\n",
        "            with open(path) as file:\n",
        "                for index, line in enumerate(file.readlines()):\n",
        "                    parts = line.split(' ', maxsplit = 2)\n",
        "                    a = int(parts[0])\n",
        "                    b = int(parts[1])\n",
        "                    color_list = parts[2].split('[', maxsplit = 1)[1].split(']', maxsplit=1)[0]\n",
        "                    colors = set(int(i) for i in color_list.split(','))\n",
        "                    graph.add_edge(a,b, colors = colors)\n",
        "            return graph\n",
        "        return nx.read_edgelist(path, comments='#', delimiter=' ', create_using=Graph,nodetype =int, data=[('color', int)])\n",
        "\n",
        "    directory = '.\\data\\small_datasets'\n",
        "    multilabel = filename.endswith('_multilabel')\n",
        "    print(\"hiii \")\n",
        "    temp = os.path\n",
        "    print (temp)\n",
        "    return [read_graph(os.path.join(directory, filename+'.edgelist'), multilabel=multilabel)]\n",
        "\n",
        "def print_cluster_head(cluster,n):\n",
        "    if len(cluster) <= n:\n",
        "        print(\"Cluster is \", cluster)\n",
        "        return\n",
        "    print(\"Cluster is \",cluster[:n], f\"... (size {len(cluster)})\")\n",
        "\n",
        "def primary_color(graph, node: int):\n",
        "    count = {}\n",
        "    for _, v in graph.edges(node):\n",
        "        for color in graph.colors_of(node, v):\n",
        "            count[color] = count.get(color, 0) + 1\n",
        "    if not count:\n",
        "        return 0\n",
        "    return max(count, key=count.get)\n",
        "\n",
        "def clean_source_code_line(line):\n",
        "    cleaned = line.split(':', maxsplit = 1)[1].split('#', maxsplit=1)[0].strip()\n",
        "    if cleaned[-1] == ',':\n",
        "        return cleaned[:-1].strip()\n",
        "    return cleaned\n",
        "\n",
        "def approx_errors(runs, graph, cluster_generator):\n",
        "    result = {\n",
        "        'errors': [],\n",
        "        'cluster_counts': [],\n",
        "        'wall_clock_times': []\n",
        "    }\n",
        "    for _ in range(runs):\n",
        "        start_time = process_time()\n",
        "        clustering = cluster_generator(graph)\n",
        "        end_time = process_time()\n",
        "        result['errors'].append(graph.error_of(clustering))\n",
        "        result['cluster_counts'].append(len(clustering))\n",
        "        result['wall_clock_times'].append(end_time - start_time)\n",
        "        print('#', end = '',flush=True)\n",
        "    print(end='\\r')\n",
        "    return result\n",
        "\n",
        "def remove_nodes_with_low_degree(graph: Graph, min_degree):\n",
        "    if min_degree == 0:\n",
        "        return\n",
        "    deleted = set()\n",
        "    pq = Q.PriorityQueue()\n",
        "    for node in graph.nodes():\n",
        "        pq.put((graph.degree[node],node))\n",
        "    while not pq.empty():\n",
        "        degree, node = pq.get()\n",
        "        if node in deleted: continue\n",
        "        if degree >= min_degree:\n",
        "            return\n",
        "        for neig in graph.neighbors(node):\n",
        "            pq.put((graph.degree[neig]-1, neig))\n",
        "        graph.remove_node(node)\n",
        "        deleted.add(node)\n",
        "\n",
        "def run_Heuristics(dataset_name, algorithms, algorithm_names):\n",
        "    print(f\"Reading dataset: {dataset_name}\")\n",
        "    dataset = read_dataset(dataset_name)\n",
        "    if MIN_DEGREE > 0:\n",
        "        dataset_name = dataset_name + f\"_min_degree_{MIN_DEGREE}\"\n",
        "    summaries = []\n",
        "    for i in range(len(algorithms)+1):\n",
        "        summary = {}\n",
        "        summary['runs'] = ITERATIONS\n",
        "        summary['number_of_nodes'] = 0\n",
        "        summary['number_of_edges'] = 0\n",
        "        summary['number_of_colors'] = 0\n",
        "        summary['dataset'] = dataset_name\n",
        "        summary['errors'] = [0]*summary['runs']\n",
        "        summary['wall_clock_times'] = [0]*summary['runs']\n",
        "        summary['number_of_clusters'] = [0]*summary['runs']\n",
        "        # summary['algorithm'] = algorithm_names[algorithms[i]].split('.', maxsplit=1)[1] if i < len(algorithms) else 'primary_edge_graph'\n",
        "        summaries.append(summary)\n",
        "    summaries[-1]['runs'] = 1 #10\n",
        "    summaries[-1]['wall_clock_times'] = [0]*summaries[-1]['runs']\n",
        "    summaries[-1]['dataset'] = dataset_name\n",
        "\n",
        "    for graph_number, graph in enumerate(dataset):\n",
        "        print(f'part {graph_number}')\n",
        "        if MIN_DEGREE > 0:\n",
        "            remove_nodes_with_low_degree(graph, MIN_DEGREE)\n",
        "\n",
        "        # measure time of to create primary_edge_graph(), we generally cache it to speed up the experiments\n",
        "        for i in range(summaries[-1]['runs']):\n",
        "            if hasattr(graph, 'primary_graph'):\n",
        "                delattr(graph, 'primary_graph')\n",
        "            start_time = process_time()\n",
        "            graph.primary_edge_graph()\n",
        "            end_time = process_time()\n",
        "            summaries[-1]['wall_clock_times'][i] += end_time - start_time\n",
        "        \n",
        "        results_by_algorithm = {}\n",
        "        for j in range(len(algorithms)):\n",
        "            summary = summaries[j]\n",
        "            alg = algorithms[j]\n",
        "            summary['number_of_nodes'] += graph.number_of_nodes()\n",
        "            summary['number_of_edges'] += graph.number_of_edges()\n",
        "            summary['number_of_colors'] = max(summary['number_of_colors'], len(graph.colors()))\n",
        "            measurements = approx_errors(summary['runs'], graph, alg)\n",
        "            for i in range(summary['runs']):\n",
        "                summary['errors'][i] += measurements['errors'][i]\n",
        "                summary['number_of_clusters'][i] += measurements['cluster_counts'][i]\n",
        "                summary['wall_clock_times'][i] += measurements['wall_clock_times'][i]\n",
        "            results_by_algorithm[algorithm_names[alg]] = mean(measurements['errors'])\n",
        "            if(j==0):\n",
        "                print(\"pivot:\")\n",
        "            elif(j==1):\n",
        "                print(\"vote:\")\n",
        "            elif(j==2):\n",
        "                print(\"vote_no_rnd:\")\n",
        "            elif(j==3):\n",
        "                print(\"vote_choose_best:\")\n",
        "            \n",
        "            \n",
        "            print(\"runs = {0:4} dataset = {1:15} mean: {2:8} median: {3:8} min: {4:8} time: {5:8} seconds\".format(summary['runs'],summary['dataset'], round(mean(measurements['errors'])), round(median(measurements['errors'])), round(min(measurements['errors'])), round(sum(measurements['wall_clock_times']), 2)))\n",
        "        print()\n",
        "\n",
        "    for i in range(len(algorithms)+1):\n",
        "        summary = summaries[i]\n",
        "        # log_real_world(summary)\n",
        "\n",
        "print(\"start running experiments...\")      \n",
        "\n",
        "algorithms = [  \n",
        "    lambda graph: pivot(graph),\n",
        "    lambda graph: vote(graph),\n",
        "    lambda graph: vote_no_rand(graph),\n",
        "    lambda graph: vote_choose_best(graph),\n",
        "]\n",
        "\n",
        "algorithm_names = {alg:clean_source_code_line(inspect.getsourcelines(alg)[0][0]) for alg in algorithms}\n",
        "\n",
        "# LSH Part################################\n",
        "\n",
        "def create_hash_func(size: int):\n",
        "    # function for creating the hash vector/function\n",
        "    hash_ex = list(range(size))\n",
        "    shuffle(hash_ex)\n",
        "    return hash_ex\n",
        "\n",
        "def build_minhash_func(vocab_size: int, nbits: int):\n",
        "    # function for building multiple minhash vectors\n",
        "    hashes = []\n",
        "    for _ in range(nbits):\n",
        "        hashes.append(create_hash_func(vocab_size))\n",
        "    return hashes\n",
        "\n",
        "def create_hash(minhash_fn, vector: list):\n",
        "    # use this function for creating our signatures (eg the matching)\n",
        "    signature = []\n",
        "    for func in minhash_fn:\n",
        "        for i in range(len(vector)):\n",
        "            idx = func.index(i)\n",
        "            signature_val = vector[idx]\n",
        "            if signature_val == 1:\n",
        "                signature.append(idx)\n",
        "                break\n",
        "    return signature\n",
        "\n",
        "def fast_score(data, cluster):\n",
        "    score = 0\n",
        "    nodes = len(data)\n",
        "    for i in (range(nodes)):\n",
        "        for j in range(i + 1, nodes):\n",
        "            # if i and j are similar and not in a same cluster\n",
        "            if data[i][j] == 1:\n",
        "                # and not in the same cluster\n",
        "                if cluster[i] != cluster[j]: score -= 1\n",
        "            # else if they are not similar\n",
        "            else:\n",
        "                # and in the same cluster\n",
        "                if cluster[i] == cluster[j]: score -= 1\n",
        "\n",
        "    return score\n",
        "\n",
        "def find_best_cluster(data, bands_clusters):\n",
        "  max = -float('inf')\n",
        "  max_cluster= None\n",
        "  for cluster in bands_clusters:\n",
        "    # scr = score(data, cluster)\n",
        "    scr = fast_score(data, cluster)\n",
        "    if scr > max:\n",
        "      max = scr\n",
        "      max_cluster = cluster\n",
        "    if scr == 0:\n",
        "      break\n",
        "  return max_cluster, max\n",
        "\n",
        "def find_best_approx_cluster(data, bands_clusters):\n",
        "  max = -float('inf')\n",
        "  max_cluster= None\n",
        "  for cluster in bands_clusters:\n",
        "    # scr = score(data, cluster)\n",
        "    scr = approx_score(data, cluster)\n",
        "    if scr > max:\n",
        "      max = scr\n",
        "      max_cluster = cluster\n",
        "    if scr == 0:\n",
        "      break\n",
        "  return max_cluster, max\n",
        "\n",
        "def approx_score(data, cluster):\n",
        "\n",
        "    score = 0\n",
        "    nodes = len(data)\n",
        "    nodes_sqrt = int(nodes ** (1/2))\n",
        "\n",
        "    # a perm. of size n\n",
        "    permutation = np.arange(nodes_sqrt).astype(int)\n",
        "    shuffle(permutation)\n",
        "    \n",
        "\n",
        "    for i in permutation:\n",
        "        for j in range(nodes):\n",
        "            # if i and j are similar and not in a same cluster\n",
        "            if data[i][j] == 1:\n",
        "                # and not in the same cluster\n",
        "                \n",
        "                if cluster[i] != cluster[j]: score -= 1\n",
        "            # else if they are not similar\n",
        "            else:\n",
        "                # and in the same cluster\n",
        "                if cluster[i] == cluster[j]: score -= 1\n",
        "\n",
        "    return score * nodes_sqrt\n",
        "\n",
        "def LSH_simmilarity_matrix_permutation_arr(num_iter, data, row_num, band_num):\n",
        "    #initialize\n",
        "    best_cluster = None\n",
        "    best_score = -float('inf')\n",
        "    nodes = len(data)\n",
        "\n",
        "    # num of iterations\n",
        "    for _ in tqdm(range(num_iter)):\n",
        "        \n",
        "        # a perm. of size n\n",
        "        permutation = np.arange(nodes).astype(int)\n",
        "        shuffle(permutation)\n",
        "\n",
        "        # new matrix with shuffled nodes\n",
        "        shuff_data = []\n",
        "        for i in range(nodes):\n",
        "            shuff_data.append([data[i][j] for j in permutation])\n",
        "        \n",
        "        \n",
        "\n",
        "        # simmilarity matrix is the new signiture\n",
        "        sigs = np.array(shuff_data)\n",
        "\n",
        "        # for each band\n",
        "        bands_clusters = []\n",
        "        for i in (range(band_num)):\n",
        "            band_cluster = [0] * nodes\n",
        "\n",
        "            # for each signiture calculate lsh and corresponding cluster\n",
        "            for j, sig in enumerate(sigs):\n",
        "                \n",
        "                lsh = int(''.join(map(str,sig[i*row_num: (i+1)*row_num])), 2) % nodes\n",
        "                \n",
        "                # band_cluster[lsh] = (band_cluster.get(lsh,list()))+ [j]\n",
        "                band_cluster[j] = lsh\n",
        "            \n",
        "            # add cluster to clusters\n",
        "            bands_clusters.append(band_cluster)\n",
        "        # print('claculating score')\n",
        "        cluster, scr = find_best_cluster(bands_clusters,data)\n",
        "        # print('score:', scr)\n",
        "        if scr > best_score:\n",
        "            best_score = scr\n",
        "            best_cluster = cluster\n",
        "        # print('best cluster:', best_cluster)\n",
        "    return best_score , best_cluster\n",
        "    # return best_score\n",
        "\n",
        "def regular_LSH(num_iter, data, row_num, band_num, sigs_num):\n",
        "  #initialize\n",
        "  best_cluster = None\n",
        "  best_score = -float('inf')\n",
        "  nodes = len(data)\n",
        "\n",
        "  for _ in tqdm(range(num_iter)):\n",
        "    # we create 20 minhash vectors\n",
        "    minhash_funcs = build_minhash_func(len(data), sigs_num)\n",
        "\n",
        "    # now create signatures\n",
        "    sigs = []\n",
        "    for i in range(len(data)):\n",
        "      sigs.append(create_hash(minhash_funcs, data[i]))\n",
        "\n",
        "    bands_clusters = []\n",
        "    for i in range(band_num):\n",
        "      band_cluster = [0] * nodes\n",
        "      for j, sig in enumerate(sigs):\n",
        "        lsh = int(''.join(map(str,sig[i*row_num: (i+1)*row_num])), 10) % nodes\n",
        "        band_cluster[j] = lsh\n",
        "      bands_clusters.append(band_cluster)\n",
        "    cluster, scr = find_best_cluster(data, bands_clusters)\n",
        "    if scr > best_score:\n",
        "      best_score = scr\n",
        "      best_cluster = cluster\n",
        "     \n",
        "  return best_score\n",
        "\n",
        "\n",
        "def LSH_singular_diff(num_iter, data, row_num, band_num, treshold, is_log = True):\n",
        "    \n",
        "    nodes = len(data) # number of nodes in dataset\n",
        "    best_cluster = [0] * nodes\n",
        "    single_nodes = [] # single nodes that have less similarity than treshold\n",
        "    for i, row in enumerate(data):\n",
        "        if sum(row) < treshold: # number of 1's in a row\n",
        "            best_cluster[i] = nodes + i    # new cluster not involve in other clusters because of node++ index\n",
        "            single_nodes.append(i)\n",
        "    # print(data, single_nodes)\n",
        "\n",
        "    # delete single elements from data\n",
        "    for i in single_nodes:\n",
        "        changed_data = np.concatenate((data[0:i, :], data[i + 1:, :]), axis=0) # delete correspondig row\n",
        "        changed_data = np.concatenate((changed_data[:,0:i], changed_data[:, i + 1:]), axis=1) # delete correspondig column\n",
        "    else:\n",
        "      changed_data = data\n",
        "    print(single_nodes)\n",
        "    print('bst:', best_cluster)\n",
        "    cluster = []\n",
        "    if len(changed_data) != 0:\n",
        "      if is_log:\n",
        "        node_log = np.floor(np.log2(len(changed_data))).astype(int)\n",
        "        row_num = node_log\n",
        "        band_num = len(changed_data) // row_num\n",
        "      else:\n",
        "        row_num = ceil(len(changed_data) / 10)\n",
        "        band_num = len(changed_data) // row_num\n",
        "        \n",
        "      # print('data',data, type(data))\n",
        "      # score, cluster = LSH_simmilarity_matrix_permutation_arr(num_iter, changed_data, row_num, band_num)\n",
        "      regular_LSH(num_iter, changed_data, row_num, band_num, 30)\n",
        "    print(cluster, best_cluster)\n",
        "    \n",
        "\n",
        "\n",
        "    #merge two clusters\n",
        "    j = 0 # index into cluster of changed_data\n",
        "    for i in range(nodes): # loop over all nodes\n",
        "       # if a node is not single meaning is not in the best cluster update it\n",
        "       if i not in single_nodes: \n",
        "          best_cluster[i] = cluster[j]\n",
        "          j += 1\n",
        "    # print(best_cluster)\n",
        "    score = fast_score(data, best_cluster)\n",
        "\n",
        "    return score, best_cluster\n",
        "\n",
        "def make_adjacecy_matrix(graph):\n",
        "    # turn nx graph to adjacency matrix\n",
        "    adjacecy_matrix = np.zeros((graph.number_of_nodes()+1,graph.number_of_nodes()+1)).astype(int)\n",
        "    np.fill_diagonal(adjacecy_matrix, 1)\n",
        "    for u,v in graph.edges():\n",
        "        adjacecy_matrix[u][v] = 1\n",
        "        adjacecy_matrix[v][u] = 1\n",
        "    return adjacecy_matrix\n",
        "\n",
        "def regulara_approx_LSH(num_iter, data, row_num, band_num, sigs_num):\n",
        "  #initialize\n",
        "  best_cluster = None\n",
        "  best_score = -float('inf')\n",
        "  nodes = len(data)\n",
        "\n",
        "  for _ in tqdm(range(num_iter)):\n",
        "    # we create 20 minhash vectors\n",
        "    minhash_funcs = build_minhash_func(len(data), sigs_num)\n",
        "\n",
        "    # now create signatures\n",
        "    sigs = []\n",
        "    for i in range(len(data)):\n",
        "      sigs.append(create_hash(minhash_funcs, data[i]))\n",
        "    # print(len(sigs), len(sigs[0]))\n",
        "    bands_clusters = []\n",
        "    # print(band_num)\n",
        "    for i in range(band_num):\n",
        "      band_cluster = [0] * nodes\n",
        "      for j, sig in enumerate(sigs):\n",
        "        # print(''.join(map(str,sig[i*row_num: (i+1)*row_num])))\n",
        "        # print(sig[i*row_num: (i+1)*row_num])\n",
        "        lsh = int(''.join(map(str,sig[i*row_num: (i+1)*row_num])), 10) #% nodes\n",
        "\n",
        "        if len(sig[i*row_num: (i+1)*row_num]) == 0: break\n",
        "        band_cluster[j] = lsh\n",
        "      bands_clusters.append(band_cluster)\n",
        "    cluster, scr = find_best_approx_cluster(data, bands_clusters)\n",
        "    if scr > best_score:\n",
        "      best_score = scr\n",
        "      best_cluster = cluster\n",
        "  return fast_score(data, best_cluster)\n",
        "    \n",
        "def run_LSH(dataset_name):\n",
        "    print(f\"Reading dataset: {dataset_name}\")\n",
        "    dataset = read_dataset(dataset_name)\n",
        "    if MIN_DEGREE > 0:\n",
        "        dataset_name = dataset_name + f\"_min_degree_{MIN_DEGREE}\"\n",
        "\n",
        "    for graph_number, graph in enumerate(dataset):\n",
        "        print(f'part {graph_number}')\n",
        "        if MIN_DEGREE > 0:\n",
        "            remove_nodes_with_low_degree(graph, MIN_DEGREE)\n",
        "        graph.primary_edge_graph()\n",
        "        adjacecy_matrix = make_adjacecy_matrix(graph)\n",
        "\n",
        "        # running sim_perm_arr LSH################################################################\n",
        "        print(\"running sim_perm_arr LSH\")\n",
        "        total_num_sigs=len(adjacecy_matrix)\n",
        "        node_log = np.floor(np.log2(len(adjacecy_matrix))).astype(int)\n",
        "        row_num = node_log\n",
        "        band_num = total_num_sigs // row_num\n",
        "        j = total_num_sigs / 20\n",
        "        start_time = process_time()\n",
        "        # score, cluster = LSH_simmilarity_matrix_permutation_arr(node_log * 5, adjacecy_matrix, row_num, band_num)\n",
        "        # score, cluster = LSH_simmilarity_matrix_permutation_arr(1, adjacecy_matrix, row_num, band_num)\n",
        "        # end_time = process_time()\n",
        "        # lsh_time = end_time - start_time\n",
        "        # output_log = 'time: ' + str(lsh_time) +' \\n row: '+ str(row_num) +' band: '+ str(band_num) +' \\n score: '+ str(score) + ' \\n number of signitures: ' +str(j)+'\\n\\n'\n",
        "        # print(output_log)\n",
        "\n",
        "        # running regular_LSH ################################################################\n",
        "        choosed_iterations = [10]\n",
        "        print(\"running regular_LSH\")\n",
        "        for num_iter in choosed_iterations:\n",
        "            print(num_iter)\n",
        "            total_num_sigs=30\n",
        "            row_num = node_log\n",
        "            band_num = ceil(total_num_sigs / row_num)\n",
        "            start_time = process_time()\n",
        "            score = regular_LSH(num_iter, adjacecy_matrix, row_num, band_num, total_num_sigs)\n",
        "            end_time = process_time()\n",
        "            lsh_time = end_time - start_time\n",
        "            output_log = 'time: ' + str(lsh_time) +' \\n row: '+ str(row_num) +' band: '+ str(band_num) +' \\n score: '+ str(score) + ' \\n number of signitures: ' +str(total_num_sigs)+'\\n\\n'\n",
        "            print(output_log)\n",
        "        \n",
        "        '''\n",
        "        # running singular_diff LSH################################################################\n",
        "        print(\"running singular_diff LSH\")\n",
        "        total_num_sigs=len(adjacecy_matrix)\n",
        "        j = total_num_sigs // 8\n",
        "        num_iter = 10\n",
        "        node_log = np.floor(np.log2(len(adjacecy_matrix))).astype(int)\n",
        "        row_num = node_log\n",
        "        band_num = total_num_sigs // row_num\n",
        "\n",
        "        score, cluster = LSH_singular_diff(num_iter, adjacecy_matrix, row_num, band_num, j, is_log = True)\n",
        "        output_log = 'dataset: '+ dataset_name +' \\n row: '+ str(row_num) +' \\n band: '+ str(band_num) +' \\n score: '+ str(score) + ' \\n number of signitures: ' +str(j)+'\\n\\n'\n",
        "        print(output_log)\n",
        "        '''\n",
        "        \n",
        "run_LSH('1000_rows')\n",
        "# run_LSH('cooking')\n",
        "# run_Heuristics('6_rows', algorithms, algorithm_names)\n",
        "# run_Heuristics('7_rows', algorithms, algorithm_names)\n",
        "# run_Heuristics('8_rows', algorithms, algorithm_names)\n",
        "# run_Heuristics('9_rows', algorithms, algorithm_names)\n",
        "# run_Heuristics('10_rows', algorithms, algorithm_names)\n",
        "# run_Heuristics('100_rows', algorithms, algorithm_names)\n",
        "# run_Heuristics('200_rows', algorithms, algorithm_names)\n",
        "# run_Heuristics('300_rows', algorithms, algorithm_names)\n",
        "# run_Heuristics('1000_rows', algorithms, algorithm_names)\n",
        "# run_Heuristics('legacy_string', algorithms, algorithm_names)\n",
        "# run_Heuristics('cooking', algorithms, algorithm_names)\n",
        "# run_Heuristics('dawn', algorithms, algorithm_names)\n",
        "# run_Heuristics('legacy_dblp', algorithms, algorithm_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "5426212241734430929ff8b86a36c8be",
            "fd306595c8c84c25984c8e8e736daed0",
            "ebf941b0e7094b86af5b885c84d7d74c",
            "1ac2a299ae13419cb3347a991fead4e8",
            "4b03771dd9c84a6a9b7b36d402a456c4",
            "0b36039246e348df93311d003e36e19e",
            "930e06542bd2427795beaf7f278555ed",
            "d0a7f2e3efdb4e17bd47b5c720bd78cb",
            "c04c8399799a4525b7b37b5cb1f5f284",
            "044e015bcbe14ba69f2ee74cc8f7a61a",
            "ce5c135443fd43a9b97247571ddeef82"
          ]
        },
        "id": "49_NuxuiZEHQ",
        "outputId": "33cac2b3-568a-435c-fdcc-445e16820b92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start running experiments...\n",
            "Reading dataset: 1000_rows\n",
            "part 0\n",
            "running sim_perm_arr LSH\n",
            "running regular_LSH\n",
            "10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5426212241734430929ff8b86a36c8be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 22.33462520399999 \n",
            " row: 9 band: 4 \n",
            " score: -100564 \n",
            " number of signitures: 30\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_LSH('cooking')"
      ],
      "metadata": {
        "id": "GeovzNbNerh6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}